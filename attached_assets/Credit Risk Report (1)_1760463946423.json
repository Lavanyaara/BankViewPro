{"id":"d91ced60-7dad-406c-b969-82a0616869fd","data":{"nodes":[{"data":{"description":"Convert Data into plain text following a specified template.","display_name":"Parse Data","id":"ParseData-tDuVW","node":{"template":{"_type":"Component","data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":true,"placeholder":"","show":true,"value":"","name":"data","display_name":"Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The data to convert to text.","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from aisandbox.custom import Component\nfrom aisandbox.helpers.data import data_to_text\nfrom aisandbox.io import DataInput, MultilineInput, Output, StrInput\nfrom aisandbox.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\", required=True, is_list=True),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n            required=True,\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        result_string = data_to_text(self.template, self.data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"sep":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"\n","name":"sep","display_name":"Separator","advanced":true,"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":true,"placeholder":"","show":true,"value":"<chunk>\nfilename:{file_name}\nembedding_id:{embedding_id}\n<file-content>\n{text}\n</file-content>\n</chunk>","name":"template","display_name":"Template","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MultilineInput"}},"description":"Convert Data into plain text following a specified template.","icon":"braces","base_classes":["Message"],"display_name":"Parse Data","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"parse_data","value":"__UNDEFINED__","cache":true}],"field_order":["data","template","sep"],"beta":false,"edited":false},"type":"ParseData"},"height":406,"id":"ParseData-tDuVW","position":{"x":761.1116666844026,"y":6.643785249633112},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":761.1116666844026,"y":6.643785249633112},"dragging":false},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-znOhl","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from aisandbox.base.prompts.api_utils import process_prompt_template\nfrom aisandbox.custom import Component\nfrom aisandbox.io import Output, PromptInput\nfrom aisandbox.schema.message import Message\nfrom aisandbox.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"{context}\n\n---\n\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nQuestion: {question}\n\nAnswer: \n\nMemory: {Memory}\n","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"prompt","_input_type":"PromptInput"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"Memory":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"Memory","display_name":"Memory","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","question","Memory"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"type":"Prompt"},"dragging":false,"height":606,"id":"Prompt-znOhl","position":{"x":1578.3038433321963,"y":-307.7560857395179},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":1578.3038433321963,"y":-307.7560857395179}},{"data":{"description":"Generate embeddings using Azure OpenAI models.","display_name":"Azure OpenAI Embeddings","id":"AzureOpenAIEmbeddings-KMgzu","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nfrom aisandbox.base.models.model import LCModelComponent\nfrom aisandbox.field_typing import Embeddings\nfrom aisandbox.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\nfrom aisandbox.utils.constants import LLM_FIELD_NAME, MODEL_NAME_TO_PROVIDER_NAME\nimport os\n\nfrom sqlmodel import select\nfrom aisandbox.services.deps import session_scope\n\n\nclass AzureOpenAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI Embeddings\"\n    description: str = \"Generate embeddings using Azure OpenAI models.\"\n    documentation: str = (\n        \"https://python.langchain.com/docs/integrations/text_embedding/azureopenai\"\n    )\n    icon = \"Azure\"\n    name = \"AzureOpenAIEmbeddings\"\n\n    default_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n    default_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n    admin_inputs = {\"azure_endpoint\": {\"display_name\": \"Azure Endpoint\", \"type\": \"string\", \"value\": default_endpoint, \"required\": True,\n                        \"info\": \"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\"},\n                    \"api_key\": {\"display_name\": \"API Key\", \"type\": \"secret_string\", \"value\": default_api_key, \"required\": True}}\n    all_models = [\"openai-text-embedding-ada-002\"]\n\n    if not os.getenv(\"EXPORT\", False):\n        from aisandbox.services.database.models.provider.model import Provider\n        provider_name = MODEL_NAME_TO_PROVIDER_NAME[name]\n        with session_scope() as session:\n            provider = session.exec(select(Provider).where((Provider.name == provider_name) & (Provider.type == \"embeddings\"))).first()\n            if provider and hasattr(provider, \"admin_settings\") and LLM_FIELD_NAME in provider.admin_settings and \"value\" in provider.admin_settings[LLM_FIELD_NAME]:\n                llm_options = provider.admin_settings.get(LLM_FIELD_NAME).get(\"value\")\n            else:\n                llm_options = []\n    else:\n        llm_options = []\n\n    inputs = [\n        DropdownInput(\n            name=LLM_FIELD_NAME,\n            display_name=\"Deployment Name\",\n            options=llm_options,\n            default_values = all_models,\n            all_possible_options=all_models,\n            value=all_models[0],\n            admin=True,\n            required=True\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.provider.model import Provider\n            with session_scope() as session:\n                provider = session.exec(select(Provider).where((Provider.name == \"Azure OpenAI Embeddings\") & (Provider.type == \"embeddings\"))).first()\n                admin_settings = provider.admin_settings\n            azure_endpoint = admin_settings.get(\"azure_endpoint\").get(\"value\")\n            api_key = admin_settings.get(\"api_key\").get(\"value\")\n        else:\n            azure_endpoint = self.default_endpoint\n            api_key = self.default_api_key\n\n        deployment_to_version = {\"openai-text-embedding-ada-002\": \"2024-02-01\"}\n        api_version = deployment_to_version.get(self.llm, \"\")\n\n        try:\n            embeddings = AzureOpenAIEmbeddings(\n                azure_endpoint=azure_endpoint,\n                azure_deployment=self.llm,\n                api_version=api_version,\n                api_key=api_key,\n                dimensions=self.dimensions or None,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Could not connect to AzureOpenAIEmbeddings API: {str(e)}\"\n            ) from e\n\n        return embeddings\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"dimensions":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"dimensions","display_name":"Dimensions","advanced":true,"dynamic":false,"info":"The number of dimensions the resulting output embeddings should have. Only supported by certain models.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"llm":{"trace_as_metadata":true,"options":["openai-text-embedding-ada-002"],"combobox":false,"required":true,"placeholder":"","show":true,"value":"openai-text-embedding-ada-002","name":"llm","display_name":"Deployment Name","advanced":false,"dynamic":false,"info":"","title_case":false,"admin":true,"hidden":false,"default_values":["openai-text-embedding-ada-002"],"all_possible_options":["openai-text-embedding-ada-002"],"type":"str","_input_type":"DropdownInput"}},"description":"Generate embeddings using Azure OpenAI models.","icon":"Azure","base_classes":["Embeddings"],"display_name":"Azure OpenAI Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/azureopenai","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["llm","dimensions"],"beta":false,"edited":false},"type":"AzureOpenAIEmbeddings"},"height":346,"id":"AzureOpenAIEmbeddings-KMgzu","position":{"x":0.7754623321125109,"y":2183.700589112122},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":0.7754623321125109,"y":2183.700589112122},"dragging":false},{"data":{"id":"ChatInput-yKBP3","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","xlsx","xls","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"admin":false,"hidden":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from aisandbox.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom aisandbox.base.io.chat import ChatComponent\nfrom aisandbox.inputs import BoolInput, StrInput\nfrom aisandbox.io import (\n    DropdownInput,\n    FileInput,\n    MessageTextInput,\n    MultilineInput,\n    Output,\n)\nfrom aisandbox.memory import store_message\nfrom aisandbox.schema.message import Message\nfrom aisandbox.utils.constants import (\n    MESSAGE_SENDER_AI,\n    MESSAGE_SENDER_USER,\n    MESSAGE_SENDER_NAME_USER,\n)\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        StrInput(\n            name=\"filenames\",\n            display_name=\"Filenames\",\n            info=\"Filenames to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            filenames=self.filenames,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n                user_id=self.graph.user_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"filenames":{"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"filenames","display_name":"Filenames","advanced":true,"dynamic":false,"info":"Filenames to be sent with the message.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","files","filenames"],"beta":false,"edited":false},"type":"ChatInput"},"height":322,"id":"ChatInput-yKBP3","position":{"x":-996.7712042853416,"y":-98.21318155226191},"selected":false,"type":"genericNode","width":384,"dragging":false,"positionAbsolute":{"x":-996.7712042853416,"y":-98.21318155226191}},{"data":{"description":"Generate embeddings using Azure OpenAI models.","display_name":"Azure OpenAI Embeddings","id":"AzureOpenAIEmbeddings-91DBt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\n\nfrom langchain_openai import AzureOpenAIEmbeddings\n\nfrom aisandbox.base.models.model import LCModelComponent\nfrom aisandbox.field_typing import Embeddings\nfrom aisandbox.io import DropdownInput, IntInput, MessageTextInput, Output, SecretStrInput\nfrom aisandbox.utils.constants import LLM_FIELD_NAME, MODEL_NAME_TO_PROVIDER_NAME\nimport os\n\nfrom sqlmodel import select\nfrom aisandbox.services.deps import session_scope\n\n\nclass AzureOpenAIEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI Embeddings\"\n    description: str = \"Generate embeddings using Azure OpenAI models.\"\n    documentation: str = (\n        \"https://python.langchain.com/docs/integrations/text_embedding/azureopenai\"\n    )\n    icon = \"Azure\"\n    name = \"AzureOpenAIEmbeddings\"\n\n    default_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n    default_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n    admin_inputs = {\"azure_endpoint\": {\"display_name\": \"Azure Endpoint\", \"type\": \"string\", \"value\": default_endpoint, \"required\": True,\n                        \"info\": \"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\"},\n                    \"api_key\": {\"display_name\": \"API Key\", \"type\": \"secret_string\", \"value\": default_api_key, \"required\": True}}\n    all_models = [\"openai-text-embedding-ada-002\"]\n\n    if not os.getenv(\"EXPORT\", False):\n        from aisandbox.services.database.models.provider.model import Provider\n        provider_name = MODEL_NAME_TO_PROVIDER_NAME[name]\n        with session_scope() as session:\n            provider = session.exec(select(Provider).where((Provider.name == provider_name) & (Provider.type == \"embeddings\"))).first()\n            if provider and hasattr(provider, \"admin_settings\") and LLM_FIELD_NAME in provider.admin_settings and \"value\" in provider.admin_settings[LLM_FIELD_NAME]:\n                llm_options = provider.admin_settings.get(LLM_FIELD_NAME).get(\"value\")\n            else:\n                llm_options = []\n    else:\n        llm_options = []\n\n    inputs = [\n        DropdownInput(\n            name=LLM_FIELD_NAME,\n            display_name=\"Deployment Name\",\n            options=llm_options,\n            default_values = all_models,\n            all_possible_options=all_models,\n            value=all_models[0],\n            admin=True,\n            required=True\n        ),\n        IntInput(\n            name=\"dimensions\",\n            display_name=\"Dimensions\",\n            info=\"The number of dimensions the resulting output embeddings should have. Only supported by certain models.\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.provider.model import Provider\n            with session_scope() as session:\n                provider = session.exec(select(Provider).where((Provider.name == \"Azure OpenAI Embeddings\") & (Provider.type == \"embeddings\"))).first()\n                admin_settings = provider.admin_settings\n            azure_endpoint = admin_settings.get(\"azure_endpoint\").get(\"value\")\n            api_key = admin_settings.get(\"api_key\").get(\"value\")\n        else:\n            azure_endpoint = self.default_endpoint\n            api_key = self.default_api_key\n\n        deployment_to_version = {\"openai-text-embedding-ada-002\": \"2024-02-01\"}\n        api_version = deployment_to_version.get(self.llm, \"\")\n\n        try:\n            embeddings = AzureOpenAIEmbeddings(\n                azure_endpoint=azure_endpoint,\n                azure_deployment=self.llm,\n                api_version=api_version,\n                api_key=api_key,\n                dimensions=self.dimensions or None,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Could not connect to AzureOpenAIEmbeddings API: {str(e)}\"\n            ) from e\n\n        return embeddings\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"dimensions":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"dimensions","display_name":"Dimensions","advanced":true,"dynamic":false,"info":"The number of dimensions the resulting output embeddings should have. Only supported by certain models.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"llm":{"trace_as_metadata":true,"options":["openai-text-embedding-ada-002"],"combobox":false,"required":true,"placeholder":"","show":true,"value":"openai-text-embedding-ada-002","name":"llm","display_name":"Deployment Name","advanced":false,"dynamic":false,"info":"","title_case":false,"admin":true,"hidden":false,"default_values":["openai-text-embedding-ada-002"],"all_possible_options":["openai-text-embedding-ada-002"],"type":"str","_input_type":"DropdownInput"}},"description":"Generate embeddings using Azure OpenAI models.","icon":"Azure","base_classes":["Embeddings"],"display_name":"Azure OpenAI Embeddings","documentation":"https://python.langchain.com/docs/integrations/text_embedding/azureopenai","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Embeddings"],"selected":"Embeddings","name":"embeddings","display_name":"Embeddings","method":"build_embeddings","value":"__UNDEFINED__","cache":true}],"field_order":["llm","dimensions"],"beta":false,"edited":false},"type":"AzureOpenAIEmbeddings"},"height":346,"id":"AzureOpenAIEmbeddings-91DBt","position":{"x":-818.7630519223042,"y":709.2478880240677},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":-818.7630519223042,"y":709.2478880240677},"dragging":false},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-0CMXD","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\nimport re\n\nfrom aisandbox.base.io.chat import ChatComponent\nfrom aisandbox.graph.vertex.utils import get_provider_model_names\nfrom aisandbox.inputs import BoolInput\nfrom aisandbox.io import DropdownInput, MessageTextInput, Output\nfrom aisandbox.memory import store_message\nfrom aisandbox.schema.message import Message\nfrom aisandbox.utils.constants import (\n    IMG_PATH_PATTERN,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n    MESSAGE_SENDER_AI,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"filenames\",\n            display_name=\"Filenames\",\n            info=\"Filenames to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        model_id, model_name = None, None\n        sender_name = self.sender_name\n        # If the text input is provided by an LLM (possibly with a Guardrails component in between), attach its id to the message\n        provider_name, model_name = get_provider_model_names(self.vertex, \"input_value\")\n        if model_name:\n            sender_name = provider_name\n            if not os.getenv(\"EXPORT\", False):\n                from aisandbox.services.database.models.model.utils import get_model_id_by_name\n                model_id = get_model_id_by_name(model_name, provider_name)\n        self.model_id = model_id\n\n        if isinstance(self.input_value, str):\n            images = re.findall(IMG_PATH_PATTERN, self.input_value)\n            self.filenames.extend(images)\n            self.input_value = re.sub(IMG_PATH_PATTERN, \"\", self.input_value)\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=sender_name,\n            session_id=self.session_id,\n            filenames=self.filenames,\n            llm_id=self.model_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n                user_id=self.graph.user_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"filenames":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"filenames","display_name":"Filenames","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filenames to be sent with the message.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","filenames","data_template"],"beta":false,"edited":false},"type":"ChatOutput"},"height":328,"id":"ChatOutput-0CMXD","position":{"x":2661.484202883699,"y":254.75401875503167},"selected":false,"type":"genericNode","width":384,"dragging":false},{"data":{"description":"Postgres Vector Store with search capabilities","display_name":"PGVector","id":"PGVector-DoOy3","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingestion Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport os\nimport re\nfrom typing import List\n\nfrom langchain_core.tools import BaseTool\nfrom langchain_postgres.vectorstores import PGVector\nfrom loguru import logger\n\nfrom aisandbox.base.vectorstores.model import LCVectorStoreComponent\nfrom aisandbox.field_typing import Tool\nfrom aisandbox.helpers.data import docs_to_data\nfrom aisandbox.io import (\n    DataInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    StrInput,\n    SecretStrInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom aisandbox.schema import Data\nfrom aisandbox.services.deps import session_scope\nfrom aisandbox.utils.constants import LLM_FIELD_NAME\nfrom aisandbox.utils.util import is_valid_uuid, remove_unsupported_db_chars\nfrom sqlmodel import select\nfrom sqlalchemy import text, inspect\n\n\nclass PostgresVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"Postgres Vector Store with search capabilities\"\n    documentation = (\n        \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    )\n    name = \"PGVector\"\n    icon = \"PGVector\"\n\n    admin_inputs = {}\n    _cached_vectorstore: PGVector | None = None\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection\",\n            required=False,\n            info=\"When empty, will default to the current flow id\",\n            input_types=[\"Text\", \"Prompt\"],\n        ),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(\n            name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=3,\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]\n        ),\n    ]\n    outputs = LCVectorStoreComponent.outputs + [\n        Output(\n            name=\"search_tool\", display_name=\"Search Tool\", method=\"build_search_tool\"\n        ),\n    ]\n\n    def _get_collection_name(self):\n        return (\n            self.collection_name\n            if is_valid_uuid(self.collection_name)\n            else f\"{self.collection_name} {self.graph.flow_id} {str(self._user_id)}\"\n        )\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        if self._cached_vectorstore:\n            return self._cached_vectorstore\n        pg_server_url = os.getenv(\"AISANDBOX_DATABASE_URL\", \"\")\n        collection_name = self._get_collection_name()\n        documents = []\n        chunk_id = 0\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                lc_doc = _input.to_lc_document()\n            else:\n                lc_doc = _input\n\n            if lc_doc.page_content:\n                lc_doc.page_content = remove_unsupported_db_chars(lc_doc.page_content)\n\n            # Extract first 3 words for chunk_name\n            text = lc_doc.page_content or \"\"\n            chunk_id += 1\n            chunk_name = \" \".join(re.split(\"\\n| \", text.strip())[:3]) + \"...\"\n            if lc_doc.metadata is None:\n                lc_doc.metadata = {}\n            lc_doc.metadata[\"chunk_name\"] = chunk_name\n            lc_doc.metadata[\"chunk_id\"] = chunk_id\n\n            documents.append(lc_doc)\n\n        if documents:\n            # Loop through the documents in chunks to avoid hitting rate limits\n            batch_size = 5\n            for i in range(0, len(documents), batch_size):\n                logger.info(f\"Processing chunk {i+1} of {len(documents)}\")\n                document_batch = documents[i : i + batch_size]\n                pgvector = PGVector.from_documents(\n                    embedding=self.embedding,\n                    documents=document_batch,  # Current batch of documents\n                    collection_name=collection_name,\n                    connection=pg_server_url,\n                    use_jsonb=True,\n                )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=collection_name,\n                connection=pg_server_url,\n                use_jsonb=True,\n            )\n        self._cached_vectorstore = pgvector\n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        # Add the id of the embedding model to the logs of the output of the PGVector component\n        model_id = None\n        for incoming_edge in self.vertex.incoming_edges:\n            if incoming_edge.target_param == \"embedding\":\n                provider_name = incoming_edge.source.display_name\n                model_name = incoming_edge.source.data[\"node\"][\"template\"][\n                    LLM_FIELD_NAME\n                ][\"value\"]\n                if not os.getenv(\"EXPORT\", False):\n                    from aisandbox.services.database.models.provider.model import (\n                        Provider,\n                    )\n                    from aisandbox.services.database.models.model.model import Model\n\n                    with session_scope() as session:\n                        provider_db = session.exec(\n                            select(Provider).where(\n                                (Provider.name == provider_name)\n                                & (Provider.type == \"embeddings\")\n                            )\n                        ).first()\n\n                        if not provider_db:\n                            raise ValueError(\n                                f\"Embedding provider not found: name='{provider_name}'\"\n                            )\n\n                        provider_id = provider_db.id\n\n                        model_db = session.exec(\n                            select(Model).where(\n                                (Model.name == model_name)\n                                & (Model.provider_id == provider_id)\n                            )\n                        ).first()\n\n                        if not model_db:\n                            raise ValueError(\n                                f\"Embedding model not found: name='{model_name}' for provider name={provider_name}\"\n                            )\n\n                        model_id = model_db.id\n        self.log({\"embedding_model_id\": model_id}, name=\"embedding_id_log\")\n\n        vector_store = self._build_pgvector()\n\n        if (\n            self.search_query\n            and isinstance(self.search_query, str)\n            and self.search_query.strip()\n        ):\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n            embedding_metadata = self.get_embedding_metadata_for_docs(docs)\n            if embedding_metadata:\n                for doc, meta in zip(docs, embedding_metadata):\n                    doc.metadata[\"embedding_id\"] = meta[\"embedding_id\"]\n\n            data = docs_to_data(docs)\n            self.status = data\n\n            return data\n        else:\n            return []\n\n    def build_search_tool(self) -> Tool:\n        \"\"\"Builds a tool for searching the vector store.\"\"\"\n\n        # Inner class to capture the component instance ('self')\n        class PGVectorSearchTool(BaseTool):\n            \"\"\"Tool for searching the PGVector store.\"\"\"\n\n            name: str = \"pgvector_search\"\n            description: str = (\n                \"Search the PGVector store for relevant documents. \"\n                \"Input should be the text query to search for. \"\n                \"Output is a string containing the concatenated content of the found documents.\"\n            )\n            vector_store_component: \"PostgresVectorStoreComponent\"  # Hold reference to the outer class instance\n\n            def _run(\n                self,\n                query: str,\n            ) -> List[Data]:\n                \"\"\"Use the tool.\"\"\"\n                if not query or not isinstance(query, str) or not query.strip():\n                    return \"Search query cannot be empty.\"\n\n                vector_store = self.vector_store_component._build_pgvector()\n                docs = vector_store.similarity_search(\n                    query=query,\n                    k=self.vector_store_component.number_of_results,\n                )\n                if not docs:\n                    return \"No results found.\"\n\n                # Concatenate document content\n                data = docs_to_data(docs)\n                data = [dat.data for dat in data]\n                return data\n\n            async def _arun(\n                self,\n                query: str,\n            ) -> List[Data]:\n                \"\"\"Use the tool asynchronously.\"\"\"\n                # Run the synchronous method in a thread pool\n                return await asyncio.to_thread(self._run, query)\n\n        # Pass the current component instance to the tool\n        return PGVectorSearchTool(vector_store_component=self)\n\n    def get_embedding_metadata_for_docs(self, docs: List) -> List[dict]:\n        doc_texts = [doc.page_content for doc in docs]\n\n        collection_name = self._get_collection_name()\n\n        embedding_meta = []\n        if not os.getenv(\"EXPORT\", False):\n            with session_scope() as session:\n                inspector = inspect(session.bind)\n\n                required_tables = {\n                    \"langchain_pg_collection\",\n                    \"langchain_pg_embedding\",\n                }\n\n                existing_tables = set(inspector.get_table_names())\n                tables_exist = required_tables.issubset(existing_tables)\n                if tables_exist:\n                    for text_content in doc_texts:\n                        result = session.execute(\n                            text(\n                                \"\"\"\n                                SELECT e.id AS embedding_id\n                                FROM langchain_pg_embedding e\n                                    JOIN langchain_pg_collection c\n                                        ON e.collection_id = c.uuid\n                                WHERE c.name = :collection AND e.document = :doc\n                                LIMIT 1\n                            \"\"\"\n                            ),\n                            {\"collection\": collection_name, \"doc\": text_content},\n                        ).first()\n\n                        embedding_meta.append(\n                            {\"embedding_id\": str(result[0]) if result else \"unknown\"}\n                        )\n\n        return embedding_meta\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"58f4e4ab-aebc-4517-9a64-257b7158a79c","name":"collection_name","display_name":"Collection","advanced":false,"input_types":["Text","Prompt"],"dynamic":false,"info":"When empty, will default to the current flow id","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":3,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MultilineInput"}},"description":"Postgres Vector Store with search capabilities","icon":"PGVector","base_classes":["Data","Retriever","Tool","VectorStore"],"display_name":"PGVector","documentation":"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true},{"types":["Tool"],"selected":"Tool","name":"search_tool","display_name":"Search Tool","method":"build_search_tool","value":"__UNDEFINED__","cache":true}],"field_order":["collection_name","search_query","ingest_data","embedding","number_of_results","embedding"],"beta":false,"edited":false},"type":"PGVector"},"height":736,"id":"PGVector-DoOy3","position":{"x":823.3540598529235,"y":1534.949789169471},"selected":false,"type":"genericNode","width":384,"dragging":false,"positionAbsolute":{"x":823.3540598529235,"y":1534.949789169471}},{"data":{"description":"Postgres Vector Store with search capabilities","display_name":"PGVector","id":"PGVector-wLR27","node":{"template":{"_type":"Component","embedding":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"embedding","display_name":"Embedding","advanced":false,"input_types":["Embeddings"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"HandleInput"},"ingest_data":{"trace_as_input":true,"trace_as_metadata":true,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"ingest_data","display_name":"Ingestion Data","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"DataInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport os\nimport re\nfrom typing import List\n\nfrom langchain_core.tools import BaseTool\nfrom langchain_postgres.vectorstores import PGVector\nfrom loguru import logger\n\nfrom aisandbox.base.vectorstores.model import LCVectorStoreComponent\nfrom aisandbox.field_typing import Tool\nfrom aisandbox.helpers.data import docs_to_data\nfrom aisandbox.io import (\n    DataInput,\n    HandleInput,\n    IntInput,\n    MultilineInput,\n    Output,\n    StrInput,\n    SecretStrInput,\n    SecretStrInput,\n    StrInput,\n)\nfrom aisandbox.schema import Data\nfrom aisandbox.services.deps import session_scope\nfrom aisandbox.utils.constants import LLM_FIELD_NAME\nfrom aisandbox.utils.util import is_valid_uuid, remove_unsupported_db_chars\nfrom sqlmodel import select\nfrom sqlalchemy import text, inspect\n\n\nclass PostgresVectorStoreComponent(LCVectorStoreComponent):\n    display_name = \"PGVector\"\n    description = \"Postgres Vector Store with search capabilities\"\n    documentation = (\n        \"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/\"\n    )\n    name = \"PGVector\"\n    icon = \"PGVector\"\n\n    admin_inputs = {}\n    _cached_vectorstore: PGVector | None = None\n    inputs = [\n        StrInput(\n            name=\"collection_name\",\n            display_name=\"Collection\",\n            required=False,\n            info=\"When empty, will default to the current flow id\",\n            input_types=[\"Text\", \"Prompt\"],\n        ),\n        MultilineInput(name=\"search_query\", display_name=\"Search Query\"),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingestion Data\",\n            is_list=True,\n        ),\n        HandleInput(\n            name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]\n        ),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            value=3,\n            advanced=True,\n        ),\n        HandleInput(\n            name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]\n        ),\n    ]\n    outputs = LCVectorStoreComponent.outputs + [\n        Output(\n            name=\"search_tool\", display_name=\"Search Tool\", method=\"build_search_tool\"\n        ),\n    ]\n\n    def _get_collection_name(self):\n        return (\n            self.collection_name\n            if is_valid_uuid(self.collection_name)\n            else f\"{self.collection_name} {self.graph.flow_id} {str(self._user_id)}\"\n        )\n\n    def build_vector_store(self) -> PGVector:\n        return self._build_pgvector()\n\n    def _build_pgvector(self) -> PGVector:\n        if self._cached_vectorstore:\n            return self._cached_vectorstore\n        pg_server_url = os.getenv(\"AISANDBOX_DATABASE_URL\", \"\")\n        collection_name = self._get_collection_name()\n        documents = []\n        chunk_id = 0\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                lc_doc = _input.to_lc_document()\n            else:\n                lc_doc = _input\n\n            if lc_doc.page_content:\n                lc_doc.page_content = remove_unsupported_db_chars(lc_doc.page_content)\n\n            # Extract first 3 words for chunk_name\n            text = lc_doc.page_content or \"\"\n            chunk_id += 1\n            chunk_name = \" \".join(re.split(\"\\n| \", text.strip())[:3]) + \"...\"\n            if lc_doc.metadata is None:\n                lc_doc.metadata = {}\n            lc_doc.metadata[\"chunk_name\"] = chunk_name\n            lc_doc.metadata[\"chunk_id\"] = chunk_id\n\n            documents.append(lc_doc)\n\n        if documents:\n            # Loop through the documents in chunks to avoid hitting rate limits\n            batch_size = 5\n            for i in range(0, len(documents), batch_size):\n                logger.info(f\"Processing chunk {i+1} of {len(documents)}\")\n                document_batch = documents[i : i + batch_size]\n                pgvector = PGVector.from_documents(\n                    embedding=self.embedding,\n                    documents=document_batch,  # Current batch of documents\n                    collection_name=collection_name,\n                    connection=pg_server_url,\n                    use_jsonb=True,\n                )\n        else:\n            pgvector = PGVector.from_existing_index(\n                embedding=self.embedding,\n                collection_name=collection_name,\n                connection=pg_server_url,\n                use_jsonb=True,\n            )\n        self._cached_vectorstore = pgvector\n        return pgvector\n\n    def search_documents(self) -> List[Data]:\n        # Add the id of the embedding model to the logs of the output of the PGVector component\n        model_id = None\n        for incoming_edge in self.vertex.incoming_edges:\n            if incoming_edge.target_param == \"embedding\":\n                provider_name = incoming_edge.source.display_name\n                model_name = incoming_edge.source.data[\"node\"][\"template\"][\n                    LLM_FIELD_NAME\n                ][\"value\"]\n                if not os.getenv(\"EXPORT\", False):\n                    from aisandbox.services.database.models.provider.model import (\n                        Provider,\n                    )\n                    from aisandbox.services.database.models.model.model import Model\n\n                    with session_scope() as session:\n                        provider_db = session.exec(\n                            select(Provider).where(\n                                (Provider.name == provider_name)\n                                & (Provider.type == \"embeddings\")\n                            )\n                        ).first()\n\n                        if not provider_db:\n                            raise ValueError(\n                                f\"Embedding provider not found: name='{provider_name}'\"\n                            )\n\n                        provider_id = provider_db.id\n\n                        model_db = session.exec(\n                            select(Model).where(\n                                (Model.name == model_name)\n                                & (Model.provider_id == provider_id)\n                            )\n                        ).first()\n\n                        if not model_db:\n                            raise ValueError(\n                                f\"Embedding model not found: name='{model_name}' for provider name={provider_name}\"\n                            )\n\n                        model_id = model_db.id\n        self.log({\"embedding_model_id\": model_id}, name=\"embedding_id_log\")\n\n        vector_store = self._build_pgvector()\n\n        if (\n            self.search_query\n            and isinstance(self.search_query, str)\n            and self.search_query.strip()\n        ):\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n            embedding_metadata = self.get_embedding_metadata_for_docs(docs)\n            if embedding_metadata:\n                for doc, meta in zip(docs, embedding_metadata):\n                    doc.metadata[\"embedding_id\"] = meta[\"embedding_id\"]\n\n            data = docs_to_data(docs)\n            self.status = data\n\n            return data\n        else:\n            return []\n\n    def build_search_tool(self) -> Tool:\n        \"\"\"Builds a tool for searching the vector store.\"\"\"\n\n        # Inner class to capture the component instance ('self')\n        class PGVectorSearchTool(BaseTool):\n            \"\"\"Tool for searching the PGVector store.\"\"\"\n\n            name: str = \"pgvector_search\"\n            description: str = (\n                \"Search the PGVector store for relevant documents. \"\n                \"Input should be the text query to search for. \"\n                \"Output is a string containing the concatenated content of the found documents.\"\n            )\n            vector_store_component: \"PostgresVectorStoreComponent\"  # Hold reference to the outer class instance\n\n            def _run(\n                self,\n                query: str,\n            ) -> List[Data]:\n                \"\"\"Use the tool.\"\"\"\n                if not query or not isinstance(query, str) or not query.strip():\n                    return \"Search query cannot be empty.\"\n\n                vector_store = self.vector_store_component._build_pgvector()\n                docs = vector_store.similarity_search(\n                    query=query,\n                    k=self.vector_store_component.number_of_results,\n                )\n                if not docs:\n                    return \"No results found.\"\n\n                # Concatenate document content\n                data = docs_to_data(docs)\n                data = [dat.data for dat in data]\n                return data\n\n            async def _arun(\n                self,\n                query: str,\n            ) -> List[Data]:\n                \"\"\"Use the tool asynchronously.\"\"\"\n                # Run the synchronous method in a thread pool\n                return await asyncio.to_thread(self._run, query)\n\n        # Pass the current component instance to the tool\n        return PGVectorSearchTool(vector_store_component=self)\n\n    def get_embedding_metadata_for_docs(self, docs: List) -> List[dict]:\n        doc_texts = [doc.page_content for doc in docs]\n\n        collection_name = self._get_collection_name()\n\n        embedding_meta = []\n        if not os.getenv(\"EXPORT\", False):\n            with session_scope() as session:\n                inspector = inspect(session.bind)\n\n                required_tables = {\n                    \"langchain_pg_collection\",\n                    \"langchain_pg_embedding\",\n                }\n\n                existing_tables = set(inspector.get_table_names())\n                tables_exist = required_tables.issubset(existing_tables)\n                if tables_exist:\n                    for text_content in doc_texts:\n                        result = session.execute(\n                            text(\n                                \"\"\"\n                                SELECT e.id AS embedding_id\n                                FROM langchain_pg_embedding e\n                                    JOIN langchain_pg_collection c\n                                        ON e.collection_id = c.uuid\n                                WHERE c.name = :collection AND e.document = :doc\n                                LIMIT 1\n                            \"\"\"\n                            ),\n                            {\"collection\": collection_name, \"doc\": text_content},\n                        ).first()\n\n                        embedding_meta.append(\n                            {\"embedding_id\": str(result[0]) if result else \"unknown\"}\n                        )\n\n        return embedding_meta\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"collection_name":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"58f4e4ab-aebc-4517-9a64-257b7158a79c","name":"collection_name","display_name":"Collection","advanced":false,"input_types":["Text","Prompt"],"dynamic":false,"info":"When empty, will default to the current flow id","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"number_of_results":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":3,"name":"number_of_results","display_name":"Number of Results","advanced":true,"dynamic":false,"info":"Number of results to return.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"search_query":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"search_query","display_name":"Search Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MultilineInput"}},"description":"Postgres Vector Store with search capabilities","icon":"PGVector","base_classes":["Data","Retriever","Tool","VectorStore"],"display_name":"PGVector","documentation":"https://python.langchain.com/v0.2/docs/integrations/vectorstores/pgvector/","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Retriever"],"selected":"Retriever","name":"base_retriever","display_name":"Retriever","method":"build_base_retriever","value":"__UNDEFINED__","cache":true},{"types":["Data"],"selected":"Data","name":"search_results","display_name":"Search Results","method":"search_documents","value":"__UNDEFINED__","cache":true},{"types":["VectorStore"],"selected":"VectorStore","name":"vector_store","display_name":"Vector Store","method":"cast_vector_store","value":"__UNDEFINED__","cache":true},{"types":["Tool"],"selected":"Tool","name":"search_tool","display_name":"Search Tool","method":"build_search_tool","value":"__UNDEFINED__","cache":true}],"field_order":["collection_name","search_query","ingest_data","embedding","number_of_results","embedding"],"beta":false,"edited":false},"type":"PGVector"},"height":736,"id":"PGVector-wLR27","position":{"x":169.72891932287212,"y":-200.76588980126098},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":169.72891932287212,"y":-200.76588980126098},"dragging":false},{"data":{"description":"Generate text using Azure OpenAI LLMs.","display_name":"Azure OpenAI","id":"AzureOpenAIModel-HAO12","node":{"template":{"_type":"Component","allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"allowed_topics","display_name":"Allowed Topics","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Only topics in this list are allowed. If empty, all topics are allowed.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_openai import AzureChatOpenAI\nfrom aisandbox.base.models.model import LCModelComponent\nfrom aisandbox.field_typing import LanguageModel\nfrom aisandbox.inputs import MessageTextInput, StrInput\nfrom aisandbox.io import DropdownInput, FloatInput, IntInput\nfrom aisandbox.utils.constants import LLM_FIELD_NAME, MODEL_NAME_TO_PROVIDER_NAME\nimport os\n\nfrom sqlmodel import select\nfrom aisandbox.services.deps import session_scope\n\n\nclass AzureChatOpenAIComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI\"\n    description: str = \"Generate text using Azure OpenAI LLMs.\"\n    documentation: str = (\n        \"https://python.langchain.com/docs/integrations/llms/azure_openai\"\n    )\n    beta = False\n    icon = \"Azure\"\n    name = \"AzureOpenAIModel\"\n\n    default_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n    default_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n    admin_inputs = {\n        \"azure_endpoint\": {\n            \"display_name\": \"Azure Endpoint\",\n            \"type\": \"string\",\n            \"value\": default_endpoint,\n            \"required\": True,\n            \"info\": \"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\",\n        },\n        \"api_key\": {\n            \"display_name\": \"API Key\",\n            \"type\": \"secret_string\",\n            \"value\": default_api_key,\n            \"required\": True,\n        },\n    }\n    all_models = [\"openai-gpt-35-turbo\", \"openai-gpt-4o\", \"openai-gpt-4o-mini\", \"openai-o1\", \"openai-o3-mini\", \"openai-gpt-5\"]\n\n    if not os.getenv(\"EXPORT\", False):\n        from aisandbox.services.database.models.model.utils import get_llm_options\n        provider_name = MODEL_NAME_TO_PROVIDER_NAME[name]\n        llm_options = get_llm_options(provider_name=provider_name, include_provider_name=False)\n    else:\n        llm_options = []\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=LLM_FIELD_NAME,\n            display_name=\"Deployment Name\",\n            options=llm_options,\n            default_values=all_models,\n            all_possible_options=all_models,\n            value=all_models[0],\n            admin=True,\n            required=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            advanced=True,\n            info=\"Controls the randomness of the model output. A higher value means the model will take more risks.\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds this value.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            advanced=True,\n            info=\"The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\",\n        ),\n        StrInput(\n            name=\"stop_sequences\",\n            display_name=\"Stop Sequences\",\n            advanced=True,\n            is_list=True,\n            info=\"The model will stop generating further tokens when it encounters any of the stop sequences. The returned text will not contain the stop sequence. The maximum allowed number of stop sequences is 4.\",\n        ),\n        FloatInput(\n            name=\"frequency_penalty\",\n            display_name=\"Frequency Penalty\",\n            advanced=True,\n            info=\"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.\",\n        ),\n        FloatInput(\n            name=\"presence_penalty\",\n            display_name=\"Presence Penalty\",\n            advanced=True,\n            info=\"Positive values penalize new tokens based on whether they appear in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.\",\n        ),\n    ]\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.provider.model import Provider\n            from aisandbox.services.database.models.model.model import Model\n            with session_scope() as session:\n                provider = session.exec(\n                    select(Provider).where(\n                        (Provider.name == \"Azure OpenAI\") & (Provider.type == \"models\")\n                    )\n                ).first()\n                admin_settings = provider.admin_settings\n                self.llm_id = session.exec(\n                    select(Model.id).where(\n                        (Model.name == self.llm) & (Model.provider_id == provider.id)\n                    )\n                ).first()\n            azure_endpoint = admin_settings.get(\"azure_endpoint\").get(\"value\")\n            api_key = admin_settings.get(\"api_key\").get(\"value\")\n        deployment_to_version = {\n            \"openai-gpt-35-turbo\": \"2024-10-21\",\n            \"openai-gpt-4o\": \"2024-10-21\",\n            \"openai-gpt-4o-mini\": \"2024-10-21\",\n            \"openai-o1\": \"2024-12-01-preview\",\n            \"openai-o3-mini\": \"2024-12-01-preview\",\n            \"openai-gpt-5\":\"2024-12-01-preview\"\n        }\n        api_version = deployment_to_version.get(self.llm, \"\")\n        stream = self.stream\n        self.max_stop_sequences_length = 4\n        temperature = self.get_config_param_value(\"temperature\")\n        top_p = self.get_config_param_value(\"top_p\")\n        if self.llm == \"openai-o1\" or self.llm == \"openai-o3-mini\":\n            temperature = None\n            top_p = None\n        try:\n            base_params = {\n                \"azure_deployment\": self.llm,\n                \"api_version\": api_version,\n                \"azure_endpoint\": (\n                    azure_endpoint\n                    if not os.getenv(\"EXPORT\", False)\n                    else self.default_endpoint\n                ),\n                \"api_key\": (\n                    api_key if not os.getenv(\"EXPORT\", False) else self.default_api_key\n                ),\n                \"streaming\": stream,\n            }\n            \n            if self.llm == \"openai-gpt-5\":\n                base_params[\"max_completion_tokens\"] = self.get_config_param_value(\"max_tokens\")\n            else:\n                if self.llm not in [\"openai-o1\", \"openai-o3-mini\"]:\n                    base_params[\"temperature\"] = temperature\n                    base_params[\"top_p\"] = top_p\n                \n                base_params.update({\n                    \"max_tokens\": self.get_config_param_value(\"max_tokens\"),\n                    \"stop\": self.get_config_param_value(\"stop_sequences\", is_list=True)[\n                        : self.max_stop_sequences_length\n                    ],\n                    \"frequency_penalty\": self.get_config_param_value(\"frequency_penalty\"),\n                    \"presence_penalty\": self.get_config_param_value(\"presence_penalty\"),\n                })\n            output = AzureChatOpenAI(**base_params)\n        except Exception as e:\n            raise ValueError(f\"Could not connect to AzureOpenAI API: {str(e)}\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"frequency_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"frequency_penalty","display_name":"Frequency Penalty","advanced":true,"dynamic":false,"info":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageInput"},"llm":{"trace_as_metadata":true,"options":["openai-gpt-4o","openai-gpt-4o-mini","openai-gpt-5","openai-o1","openai-o3-mini"],"combobox":false,"required":true,"placeholder":"","show":true,"value":"openai-gpt-4o","name":"llm","display_name":"Deployment Name","advanced":false,"dynamic":false,"info":"","title_case":false,"admin":true,"hidden":false,"default_values":["openai-gpt-35-turbo","openai-gpt-4o","openai-gpt-4o-mini","openai-o1","openai-o3-mini","openai-gpt-5"],"all_possible_options":["openai-gpt-35-turbo","openai-gpt-4o","openai-gpt-4o-mini","openai-o1","openai-o3-mini","openai-gpt-5"],"type":"str","_input_type":"DropdownInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds this value.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"dict","_input_type":"DictInput"},"not_allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"not_allowed_topics","display_name":"Not Allowed Topics","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Topics in this list are explicitly not allowed. This field takes precedence over the allowed topics.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"presence_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"presence_penalty","display_name":"Presence Penalty","advanced":true,"dynamic":false,"info":"Positive values penalize new tokens based on whether they appear in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"stop_sequences":{"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"stop_sequences","display_name":"Stop Sequences","advanced":true,"dynamic":false,"info":"The model will stop generating further tokens when it encounters any of the stop sequences. The returned text will not contain the stop sequence. The maximum allowed number of stop sequences is 4.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"store_analytics":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_analytics","display_name":"Store Analytics","advanced":true,"dynamic":false,"info":"Whether or not to keep track of LLM analytics.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"admin":false,"hidden":true,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"temperature","display_name":"Temperature","advanced":true,"dynamic":false,"info":"Controls the randomness of the model output. A higher value means the model will take more risks.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"}},"description":"Generate text using Azure OpenAI LLMs.","icon":"Azure","base_classes":["LanguageModel","Message"],"display_name":"Azure OpenAI","documentation":"https://python.langchain.com/docs/integrations/llms/azure_openai","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","allowed_topics","not_allowed_topics","stream","store_analytics","model_kwargs","llm","temperature","max_tokens","top_p","stop_sequences","frequency_penalty","presence_penalty"],"beta":false,"edited":false},"type":"AzureOpenAIModel"},"height":480,"id":"AzureOpenAIModel-HAO12","position":{"x":2043.2248504421104,"y":186.56029226522503},"selected":false,"type":"genericNode","width":384},{"data":{"description":"An S3 file loader for text-based files.","display_name":"S3 File","id":"S3File-67R8g","node":{"template":{"_type":"Component","path":{"trace_as_metadata":true,"file_path":"d91ced60-7dad-406c-b969-82a0616869fd/final_2024_digital_tagged_annual_report.pdf","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","xlsx","xls"],"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"path","display_name":"Path","advanced":false,"dynamic":false,"info":"Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx, xlsx, xls","title_case":false,"admin":false,"hidden":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from pathlib import Path\n\nfrom aisandbox.base.data.utils import TEXT_FILE_TYPES, parse_text_file_to_data\nfrom aisandbox.custom import Component\nfrom aisandbox.io import BoolInput, FileInput, Output, MessageTextInput\nfrom aisandbox.schema import Data\nimport tempfile\nimport os\nfrom aisandbox.services.deps import get_storage_service, session_scope\n\n\nclass S3FileComponent(Component):\n    display_name = \"S3 File\"\n    description = \"An S3 file loader for text-based files.\"\n    icon = \"Amazon\"\n    name = \"S3File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Path\",\n            file_types=TEXT_FILE_TYPES,\n            info=f\"Supported file types: {', '.join(TEXT_FILE_TYPES)}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        MessageTextInput(\n            name=\"label\",\n            display_name=\"Label\",\n            info=\"Label for reference in the Playground.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"load_file\"),\n    ]\n\n    async def load_file(self) -> Data:\n        if not self.path:\n            raise ValueError(\"Please, upload a file to use this component.\")\n        resolved_path = self.resolve_path(self.path)\n        silent_errors = self.silent_errors\n\n        extension = Path(resolved_path).suffix[1:].lower()\n\n        if extension == \"doc\":\n            raise ValueError(\"doc files are not supported. Please save as .docx\")\n        if extension not in TEXT_FILE_TYPES:\n            raise ValueError(f\"Unsupported file type: {extension}\")\n\n        folder = os.path.dirname(self.path)\n        file_name = os.path.basename(self.path)\n\n        file_bytes = await get_storage_service().get_file(folder, file_name)\n        with tempfile.TemporaryDirectory() as temp_dir:\n            file_path = os.path.join(temp_dir, os.path.basename(self.path))\n            with open(file_path, \"wb\") as f:\n                f.write(file_bytes)\n            data = parse_text_file_to_data(file_path, silent_errors)\n\n        # Log uploaded file\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.file.crud import log_file\n            from aisandbox.services.database.models.file.model import FilePurpose\n\n            with session_scope() as session:\n                log_file(\n                    session=session,\n                    name=file_name,\n                    s3_path=self.path,\n                    purpose=FilePurpose.FILE_COMPONENT,\n                    created_by=self.graph.user_id,\n                    description=\"Uploaded via S3FileComponent\",\n                )\n\n        self.status = data if data else \"No data\"\n        return data or Data()\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"label":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"label","display_name":"Label","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Label for reference in the Playground.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"silent_errors":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"silent_errors","display_name":"Silent Errors","advanced":true,"dynamic":false,"info":"If true, errors will not raise an exception.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"}},"description":"An S3 file loader for text-based files.","icon":"Amazon","base_classes":["Data"],"display_name":"S3 File","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"load_file","value":"__UNDEFINED__","cache":true}],"field_order":["path","silent_errors","label"],"beta":false,"edited":false},"type":"S3File"},"height":414,"id":"S3File-67R8g","position":{"x":-590.4894538340119,"y":1281.405072720288},"selected":false,"type":"genericNode","width":384,"dragging":false,"positionAbsolute":{"x":-590.4894538340119,"y":1281.405072720288}},{"data":{"id":"RecursiveCharacterTextSplitter-TybcH","node":{"template":{"_type":"Component","data_input":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"data_input","display_name":"Data Inputs","advanced":false,"input_types":["Document","Data"],"dynamic":false,"info":"The data to split.","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"DataInput"},"chunk_overlap":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":200,"name":"chunk_overlap","display_name":"Chunk Overlap","advanced":false,"dynamic":false,"info":"Number of characters to overlap between chunks.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"chunk_size":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1000,"name":"chunk_size","display_name":"Chunk Size","advanced":false,"dynamic":false,"info":"The maximum number of characters in each chunk.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter, TextSplitter\nfrom aisandbox.base.textsplitters.model import LCTextSplitterComponent\nfrom aisandbox.inputs.inputs import DataInput, IntInput, MessageTextInput\nfrom aisandbox.utils.util import unescape_string\n\n\nclass RecursiveCharacterTextSplitterComponent(LCTextSplitterComponent):\n    display_name: str = \"Recursive Text Splitter\"\n    description: str = \"Split text into chunks of a specified length based on the speficied separators.\"\n    name = \"RecursiveCharacterTextSplitter\"\n\n    inputs = LCTextSplitterComponent._base_inputs + [\n        MessageTextInput(\n            name=\"separators\",\n            display_name=\"Separators\",\n            info='The characters to split on. If left empty defaults to [\"\\n\\n\", \"\\n\", \" \", \"\"].',\n            is_list=True,\n        ),\n    ]\n\n    def get_data_input(self) -> Any:\n        return self.data_input\n\n    def build_text_splitter(self) -> TextSplitter:\n        if not self.separators:\n            separators: list[str] | None = None\n        else:\n            # check if the separators list has escaped characters\n            # if there are escaped characters, unescape them\n            separators = [unescape_string(x) for x in self.separators]\n\n        return RecursiveCharacterTextSplitter(\n            separators=separators,\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap,\n        )\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"separators":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"separators","display_name":"Separators","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The characters to split on. If left empty defaults to [\"\n\n\", \"\n\", \" \", \"\"].","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Split text into chunks of a specified length based on the speficied separators.","base_classes":["Data"],"display_name":"Recursive Text Splitter","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"split_data","value":"__UNDEFINED__","cache":true}],"field_order":["data_input","chunk_size","chunk_overlap","separators"],"beta":false,"edited":false},"type":"RecursiveCharacterTextSplitter"},"dragging":false,"height":587,"id":"RecursiveCharacterTextSplitter-TybcH","position":{"x":33.02508324302289,"y":1228.5251297887035},"positionAbsolute":{"x":33.02508324302289,"y":1228.5251297887035},"selected":false,"type":"genericNode","width":384},{"id":"Memory-EElXY","type":"genericNode","position":{"x":923.7232201345457,"y":-672.9150049602966},"data":{"type":"Memory","node":{"template":{"_type":"Component","memory":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"memory","display_name":"External Memory","advanced":false,"input_types":["BaseChatMessageHistory"],"dynamic":false,"info":"Retrieve messages from an external memory. If empty, it will use the database.","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\nfrom aisandbox.custom import Component\nfrom aisandbox.helpers.data import data_to_text\nfrom aisandbox.inputs import HandleInput\nfrom aisandbox.io import DropdownInput, IntInput, MessageTextInput, MultilineInput, Output\nfrom aisandbox.memory import get_messages, LCBuiltinChatMemory\nfrom aisandbox.schema import Data\nfrom aisandbox.schema.message import Message\nfrom aisandbox.field_typing import BaseChatMemory\nfrom langchain.memory import ConversationBufferMemory\n\nfrom aisandbox.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, LLM_FIELD_NAME\n\nclass MemoryComponent(Component):\n    display_name = \"Chat Memory\"\n    description = \"Retrieves stored chat messages from the database or from an external memory.\"\n    icon = \"message-square-more\"\n    name = \"Memory\"\n\n    MACHINE_AND_USER_LABEL = \"Machine and User\"\n\n    llm_options = [\"\"]\n    if not os.getenv(\"EXPORT\", False):\n        from aisandbox.services.database.models.model.utils import get_llm_options\n        llm_options = get_llm_options()\n\n\n    inputs = [\n        HandleInput(\n            name=\"memory\",\n            display_name=\"External Memory\",\n            input_types=[\"BaseChatMessageHistory\"],\n            info=\"Retrieve messages from an external memory. If empty, it will use the database.\",\n        ),\n        DropdownInput(\n            name=LLM_FIELD_NAME,\n            display_name=\"Large language model ID\",\n            options=llm_options,\n            value=\"\",\n            info=\"Filter by large language model.\",\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MACHINE_AND_USER_LABEL],\n            value=MACHINE_AND_USER_LABEL,\n            info=\"Filter by sender type.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Filter by sender name.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"n_messages\",\n            display_name=\"Number of Messages\",\n            value=100,\n            info=\"Number of messages to retrieve.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"order\",\n            display_name=\"Order\",\n            options=[\"Ascending\", \"Descending\"],\n            value=\"Ascending\",\n            info=\"Order of the messages.\",\n            advanced=True,\n        ),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.\",\n            value=\"{sender_name}: {text}\",\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Messages (Data)\", name=\"messages\", method=\"retrieve_messages\"),\n        Output(display_name=\"Messages (Text)\", name=\"messages_text\", method=\"retrieve_messages_as_text\"),\n        Output(display_name=\"Memory\", name=\"lc_memory\", method=\"build_lc_memory\"),\n    ]\n\n    def retrieve_messages(self) -> Data:\n        sender = self.sender\n        sender_name = self.sender_name\n        session_id = self.session_id\n        n_messages = self.n_messages\n        order = \"DESC\" if self.order == \"Descending\" else \"ASC\"\n\n        if sender == self.MACHINE_AND_USER_LABEL:\n            sender = None\n\n        llm_id = None\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.model.utils import get_llm_options\n            llm_options = get_llm_options(dict_with_ids=True)\n            if self.llm:\n                llm_id = llm_options.get(self.llm)\n\n        if self.memory:\n            # override session_id\n            self.memory.session_id = session_id\n\n            stored = self.memory.messages\n            if order == \"ASC\":\n                stored = stored[::-1]\n            if n_messages:\n                stored = stored[:n_messages]\n            stored = [Message.from_lc_message(m) for m in stored]\n            if sender:\n                expected_type = MESSAGE_SENDER_AI if sender == MESSAGE_SENDER_AI else MESSAGE_SENDER_USER\n                stored = [m for m in stored if m.type == expected_type]\n        else:\n            if not os.getenv(\"EXPORT\", False):\n                stored = get_messages(\n                    sender=sender,\n                    sender_name=sender_name,\n                    session_id=session_id,\n                    limit=n_messages,\n                    order=order,\n                    llm_id=llm_id,\n                )\n            else:\n                stored =[]\n            stored_cap = []\n            token_count = 0\n            max_memory_tokens = 4096\n            for message in reversed(stored):\n                num_tokens = 1.3 * len(message.text.split(\" \"))\n                token_count += num_tokens\n                if token_count > max_memory_tokens:\n                    break\n                stored_cap.append(message)\n            stored_cap = stored_cap[::-1]\n        self.status = stored_cap\n        return stored_cap\n\n    def retrieve_messages_as_text(self) -> Message:\n        stored_text = data_to_text(self.template, self.retrieve_messages())\n        self.status = stored_text\n        return Message(text=stored_text)\n\n    def build_lc_memory(self) -> BaseChatMemory:\n        if self.memory:\n            chat_memory = self.memory\n        else:\n            chat_memory = LCBuiltinChatMemory(flow_id=self.graph.flow_id, session_id=self.session_id)\n        return ConversationBufferMemory(chat_memory=chat_memory)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"llm":{"trace_as_metadata":true,"options":["Amazon Bedrock amazon.nova-lite-v1:0","Amazon Bedrock amazon.nova-micro-v1:0","Amazon Bedrock amazon.nova-pro-v1:0","Amazon Bedrock amazon.titan-text-express-v1","Amazon Bedrock anthropic.claude-3-7-sonnet-20250219-v1:0","Amazon Bedrock anthropic.claude-opus-4-20250514-v1:0","Amazon Bedrock anthropic.claude-sonnet-4-20250514-v1:0","Amazon Bedrock deepseek.r1-v1:0","Amazon Bedrock meta.llama3-70b-instruct-v1:0","Amazon Bedrock mistral.mistral-7b-instruct-v0:2","Azure AI Foundry DeepSeek-R1","Azure AI Foundry Image dall-e-3","Azure AI Foundry Image gpt-image-1","Azure AI Foundry gpt-oss-120b","Azure OpenAI openai-gpt-4o","Azure OpenAI openai-gpt-4o-mini","Azure OpenAI openai-gpt-5","Azure OpenAI openai-o1","Azure OpenAI openai-o3-mini","HuggingFace TGI Phi-3.5-instruct","Snowflake Cortex llama3.1-405b","Snowflake Cortex llama3.1-70b","Snowflake Cortex llama3.1-8b","Snowflake Cortex mistral-7b"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"","name":"llm","display_name":"Large language model ID","advanced":false,"dynamic":false,"info":"Filter by large language model.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"n_messages":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":100,"name":"n_messages","display_name":"Number of Messages","advanced":true,"dynamic":false,"info":"Number of messages to retrieve.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"order":{"trace_as_metadata":true,"options":["Ascending","Descending"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Ascending","name":"order","display_name":"Order","advanced":true,"dynamic":false,"info":"Order of the messages.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User","Machine and User"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Machine and User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Filter by sender type.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filter by sender name.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"template":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{sender_name}: {text}","name":"template","display_name":"Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The template to use for formatting the data. It can contain the keys {text}, {sender} or any other key in the message data.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MultilineInput"},"edit":false},"description":"Retrieves stored chat messages from the database or from an external memory.","icon":"message-square-more","base_classes":["BaseChatMemory","Data","Message"],"display_name":"Chat Memory","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"messages","display_name":"Messages (Data)","method":"retrieve_messages","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"messages_text","display_name":"Messages (Text)","method":"retrieve_messages_as_text","value":"__UNDEFINED__","cache":true},{"types":["BaseChatMemory"],"selected":"BaseChatMemory","name":"lc_memory","display_name":"Memory","method":"build_lc_memory","value":"__UNDEFINED__","cache":true}],"field_order":["memory","llm","sender","sender_name","n_messages","session_id","order","template"],"beta":false,"edited":false},"id":"Memory-EElXY"},"selected":false,"width":384,"height":538,"positionAbsolute":{"x":923.7232201345457,"y":-672.9150049602966},"dragging":false},{"id":"Guardrails-OBiSc","type":"genericNode","position":{"x":-348.3838145805454,"y":-28.20891635230626},"data":{"type":"Guardrails","node":{"template":{"_type":"Component","llm":{"trace_as_metadata":true,"list":false,"required":true,"placeholder":"","show":true,"value":"","name":"llm","display_name":"Language Model","advanced":false,"input_types":["LanguageModel"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"other","_input_type":"HandleInput"},"allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"allowed_topics","display_name":"Allowed Topics","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Only topics in this list are allowed. If empty, all topics are allowed.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from aisandbox.custom import Component\nfrom aisandbox.io import MessageTextInput, Output\nfrom aisandbox.schema import Data\nfrom aisandbox.schema.message import Message\nfrom aisandbox.inputs import HandleInput, MessageTextInput, BoolInput, DropdownInput, StrInput\nfrom aisandbox.utils.constants import GUARDRAIL_RESTRICTIONS_MESSAGE\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom loguru import logger\n\n\nclass Guardrails(Component):\n    display_name = \"Guardrails\"\n    description = \"Evaluates text against allowed and not allowed criteria. Note that if it's placed between a model component and a Chat Output component, the model won't be able to stream.\"\n    icon = \"shield\"\n    name = \"Guardrails\"\n    _cached_is_allowed: str | None = None\n    _cached_is_allowed_text: str | None = None\n\n    inputs = [\n        MessageTextInput(\n            name=\"allowed_topics\",\n            display_name=\"Allowed Topics\",\n            info=\"Only topics in this list are allowed. If empty, all topics are allowed.\",\n            is_list=True\n        ),\n        MessageTextInput(\n            name=\"not_allowed_topics\",\n            display_name=\"Not Allowed Topics\",\n            info=\"Topics in this list are explicitly not allowed. This field takes precedence over the allowed topics.\",\n            is_list=True\n        ),\n        MessageTextInput(\n            name=\"context\",\n            display_name=\"Context\",\n            info=\"Context for evaluation.\",\n            advanced=True\n        ),\n        MessageTextInput(\n            name=\"text\",\n            display_name=\"Text to Evaluate\",\n            info=\"Prompt or response text to be evaluated.\"\n        ),\n        HandleInput(name=\"llm\", display_name=\"Language Model\", input_types=[\"LanguageModel\"], required=True),\n    ]\n\n    outputs = [\n        Output(display_name=\"Potentially Redacted Text\", name=\"redacted_text\", method=\"general_response\"),\n        Output(display_name=\"Guardrails Decision\", name=\"guardrails_decision\", method=\"evaluate_text\"),\n        Output(display_name=\"True Route\", name=\"true_result\", method=\"true_response\"),\n        Output(display_name=\"False Route\", name=\"false_result\", method=\"false_response\"),\n    ]\n\n    def evaluate_text(self) -> Message:\n        # Split the allowed and not_allowed strings into lists\n        allowed_topics = [topic for topic in self.allowed_topics if topic]\n        not_allowed_topics = [topic for topic in self.not_allowed_topics if topic]\n\n        # Create a system message to guide the LLM\n        system_message = SystemMessage(content=(\n            \"You are a guardrail evaluator. Your task is to assess whether the given text adheres to the specified allowed \"\n            \"topics and avoids the specified not allowed topics, taking into account the given context if it is provided. \"\n            \"Answer with 'True' or 'False'. If the text is a question that would require an answer including a topic in \"\n            \"the not allowed topics, answer 'False'. In addition, always explain your reasoning.\"\n        ))\n\n        # Combine inputs into a single human message\n        prompt = f\"Context: {self.context}\\n\" if self.context else \"\"\n        if allowed_topics:\n            prompt += f\"Allowed topics: {', '.join(allowed_topics)}\\n\"\n        if not_allowed_topics:\n            prompt += f\"Not allowed topics: {', '.join(not_allowed_topics)}\\n\"\n        prompt += f\"Text: {self.text}\\n\\n\"\n        prompt += \"Does the above text adhere to the allowed topics and avoid the not allowed topics?\"\n        human_message = HumanMessage(content=prompt)\n\n        try:\n            if (not allowed_topics and not not_allowed_topics):\n                self._cached_is_allowed = True\n                llm_response = \"True.\\n\\nThe given text is in adherence with the given topic restrictions, of which there are none.\"\n            elif self.text == GUARDRAIL_RESTRICTIONS_MESSAGE:\n                self._cached_is_allowed = False\n                llm_response = \"False.\\n\\nThe given text is itself already a guardrail restrictions message.\"\n            else:\n                # Call the LLM to evaluate the text\n                llm_response = self.llm.invoke([system_message, human_message])\n                llm_response = llm_response.content if hasattr(llm_response, \"content\") else llm_response\n\n                # Extract the response and parse it\n                self._cached_is_allowed = \"true\" in llm_response.strip().lower()\n            self._cached_is_allowed_text = llm_response\n\n            self.status = llm_response\n            return llm_response\n        except Exception as e:\n            logger.error(f\"Error during LLM invocation: {str(e)}\")\n            raise ValueError(f\"Error during LLM invocation: {str(e)}\") from e\n\n    def general_response(self) -> Message:\n        if not self._cached_is_allowed_text:\n            self.evaluate_text()\n        if self._cached_is_allowed:\n            response = self.text\n        else:\n            response = GUARDRAIL_RESTRICTIONS_MESSAGE\n        self.status = response\n        return response\n\n    def true_response(self) -> Message:\n        if not self._cached_is_allowed_text:\n            self.evaluate_text()\n        if self._cached_is_allowed:\n            self.status = self.text\n            return self.text\n        else:\n            self.stop(\"true_result\")\n            return None\n\n    def false_response(self) -> Message:\n        if not self._cached_is_allowed_text:\n            self.evaluate_text()\n        if not self._cached_is_allowed:\n            self.status = GUARDRAIL_RESTRICTIONS_MESSAGE\n            return GUARDRAIL_RESTRICTIONS_MESSAGE\n        else:\n            self.stop(\"false_result\")\n            return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"context":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"context","display_name":"Context","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Context for evaluation.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"not_allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["Politics, Cinema, NSFW, Adult Content, Violence, Drugs"],"name":"not_allowed_topics","display_name":"Not Allowed Topics","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Topics in this list are explicitly not allowed. This field takes precedence over the allowed topics.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"text":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"text","display_name":"Text to Evaluate","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Prompt or response text to be evaluated.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"edit":true},"description":"Evaluates text against allowed and not allowed criteria. Note that if it's placed between a model component and a Chat Output component, the model won't be able to stream.","icon":"shield","base_classes":["Message"],"display_name":"Guardrails","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"redacted_text","display_name":"Potentially Redacted Text","method":"general_response","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"guardrails_decision","display_name":"Guardrails Decision","method":"evaluate_text","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"true_result","display_name":"True Route","method":"true_response","value":"__UNDEFINED__","cache":true},{"types":["Message"],"selected":"Message","name":"false_result","display_name":"False Route","method":"false_response","value":"__UNDEFINED__","cache":true}],"field_order":["allowed_topics","not_allowed_topics","context","text","llm"],"beta":false,"edited":false},"id":"Guardrails-OBiSc"},"selected":false,"width":384,"height":859,"positionAbsolute":{"x":-348.3838145805454,"y":-28.20891635230626},"dragging":false},{"id":"ChatOutput-2jCcY","type":"genericNode","position":{"x":1751.057684701959,"y":806.1147416861583},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\nimport re\n\nfrom aisandbox.base.io.chat import ChatComponent\nfrom aisandbox.graph.vertex.utils import get_provider_model_names\nfrom aisandbox.inputs import BoolInput\nfrom aisandbox.io import DropdownInput, MessageTextInput, Output\nfrom aisandbox.memory import store_message\nfrom aisandbox.schema.message import Message\nfrom aisandbox.utils.constants import (\n    IMG_PATH_PATTERN,\n    MESSAGE_SENDER_NAME_AI,\n    MESSAGE_SENDER_USER,\n    MESSAGE_SENDER_AI,\n)\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"filenames\",\n            display_name=\"Filenames\",\n            info=\"Filenames to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        model_id, model_name = None, None\n        sender_name = self.sender_name\n        # If the text input is provided by an LLM (possibly with a Guardrails component in between), attach its id to the message\n        provider_name, model_name = get_provider_model_names(self.vertex, \"input_value\")\n        if model_name:\n            sender_name = provider_name\n            if not os.getenv(\"EXPORT\", False):\n                from aisandbox.services.database.models.model.utils import get_model_id_by_name\n                model_id = get_model_id_by_name(model_name, provider_name)\n        self.model_id = model_id\n\n        if isinstance(self.input_value, str):\n            images = re.findall(IMG_PATH_PATTERN, self.input_value)\n            self.filenames.extend(images)\n            self.input_value = re.sub(IMG_PATH_PATTERN, \"\", self.input_value)\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=sender_name,\n            session_id=self.session_id,\n            filenames=self.filenames,\n            llm_id=self.model_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n                user_id=self.graph.user_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"filenames":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"filenames","display_name":"Filenames","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Filenames to be sent with the message.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"admin":false,"hidden":false,"default_values":[],"all_possible_options":[],"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"},"edit":false},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","filenames","data_template"],"beta":false,"edited":false},"id":"ChatOutput-2jCcY"},"selected":false,"width":384,"height":328},{"id":"AzureOpenAIModel-5CKr3","type":"genericNode","position":{"x":495.8792050692686,"y":818.9789427951316},"data":{"type":"AzureOpenAIModel","node":{"template":{"_type":"Component","allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":[],"name":"allowed_topics","display_name":"Allowed Topics","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Only topics in this list are allowed. If empty, all topics are allowed.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_openai import AzureChatOpenAI\nfrom aisandbox.base.models.model import LCModelComponent\nfrom aisandbox.field_typing import LanguageModel\nfrom aisandbox.inputs import MessageTextInput, StrInput\nfrom aisandbox.io import DropdownInput, FloatInput, IntInput\nfrom aisandbox.utils.constants import LLM_FIELD_NAME, MODEL_NAME_TO_PROVIDER_NAME\nimport os\n\nfrom sqlmodel import select\nfrom aisandbox.services.deps import session_scope\n\n\nclass AzureChatOpenAIComponent(LCModelComponent):\n    display_name: str = \"Azure OpenAI\"\n    description: str = \"Generate text using Azure OpenAI LLMs.\"\n    documentation: str = (\n        \"https://python.langchain.com/docs/integrations/llms/azure_openai\"\n    )\n    beta = False\n    icon = \"Azure\"\n    name = \"AzureOpenAIModel\"\n\n    default_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\", \"\")\n    default_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\", \"\")\n    admin_inputs = {\n        \"azure_endpoint\": {\n            \"display_name\": \"Azure Endpoint\",\n            \"type\": \"string\",\n            \"value\": default_endpoint,\n            \"required\": True,\n            \"info\": \"Your Azure endpoint, including the resource. Example: `https://example-resource.azure.openai.com/`\",\n        },\n        \"api_key\": {\n            \"display_name\": \"API Key\",\n            \"type\": \"secret_string\",\n            \"value\": default_api_key,\n            \"required\": True,\n        },\n    }\n    all_models = [\"openai-gpt-35-turbo\", \"openai-gpt-4o\", \"openai-gpt-4o-mini\", \"openai-o1\", \"openai-o3-mini\", \"openai-gpt-5\"]\n\n    if not os.getenv(\"EXPORT\", False):\n        from aisandbox.services.database.models.model.utils import get_llm_options\n        provider_name = MODEL_NAME_TO_PROVIDER_NAME[name]\n        llm_options = get_llm_options(provider_name=provider_name, include_provider_name=False)\n    else:\n        llm_options = []\n\n    inputs = LCModelComponent._base_inputs + [\n        DropdownInput(\n            name=LLM_FIELD_NAME,\n            display_name=\"Deployment Name\",\n            options=llm_options,\n            default_values=all_models,\n            all_possible_options=all_models,\n            value=all_models[0],\n            admin=True,\n            required=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            advanced=True,\n            info=\"Controls the randomness of the model output. A higher value means the model will take more risks.\",\n        ),\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds this value.\",\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            advanced=True,\n            info=\"The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\",\n        ),\n        StrInput(\n            name=\"stop_sequences\",\n            display_name=\"Stop Sequences\",\n            advanced=True,\n            is_list=True,\n            info=\"The model will stop generating further tokens when it encounters any of the stop sequences. The returned text will not contain the stop sequence. The maximum allowed number of stop sequences is 4.\",\n        ),\n        FloatInput(\n            name=\"frequency_penalty\",\n            display_name=\"Frequency Penalty\",\n            advanced=True,\n            info=\"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.\",\n        ),\n        FloatInput(\n            name=\"presence_penalty\",\n            display_name=\"Presence Penalty\",\n            advanced=True,\n            info=\"Positive values penalize new tokens based on whether they appear in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.\",\n        ),\n    ]\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        if not os.getenv(\"EXPORT\", False):\n            from aisandbox.services.database.models.provider.model import Provider\n            from aisandbox.services.database.models.model.model import Model\n            with session_scope() as session:\n                provider = session.exec(\n                    select(Provider).where(\n                        (Provider.name == \"Azure OpenAI\") & (Provider.type == \"models\")\n                    )\n                ).first()\n                admin_settings = provider.admin_settings\n                self.llm_id = session.exec(\n                    select(Model.id).where(\n                        (Model.name == self.llm) & (Model.provider_id == provider.id)\n                    )\n                ).first()\n            azure_endpoint = admin_settings.get(\"azure_endpoint\").get(\"value\")\n            api_key = admin_settings.get(\"api_key\").get(\"value\")\n        deployment_to_version = {\n            \"openai-gpt-35-turbo\": \"2024-10-21\",\n            \"openai-gpt-4o\": \"2024-10-21\",\n            \"openai-gpt-4o-mini\": \"2024-10-21\",\n            \"openai-o1\": \"2024-12-01-preview\",\n            \"openai-o3-mini\": \"2024-12-01-preview\",\n            \"openai-gpt-5\":\"2024-12-01-preview\"\n        }\n        api_version = deployment_to_version.get(self.llm, \"\")\n        stream = self.stream\n        self.max_stop_sequences_length = 4\n        temperature = self.get_config_param_value(\"temperature\")\n        top_p = self.get_config_param_value(\"top_p\")\n        if self.llm == \"openai-o1\" or self.llm == \"openai-o3-mini\":\n            temperature = None\n            top_p = None\n        try:\n            base_params = {\n                \"azure_deployment\": self.llm,\n                \"api_version\": api_version,\n                \"azure_endpoint\": (\n                    azure_endpoint\n                    if not os.getenv(\"EXPORT\", False)\n                    else self.default_endpoint\n                ),\n                \"api_key\": (\n                    api_key if not os.getenv(\"EXPORT\", False) else self.default_api_key\n                ),\n                \"streaming\": stream,\n            }\n            \n            if self.llm == \"openai-gpt-5\":\n                base_params[\"max_completion_tokens\"] = self.get_config_param_value(\"max_tokens\")\n            else:\n                if self.llm not in [\"openai-o1\", \"openai-o3-mini\"]:\n                    base_params[\"temperature\"] = temperature\n                    base_params[\"top_p\"] = top_p\n                \n                base_params.update({\n                    \"max_tokens\": self.get_config_param_value(\"max_tokens\"),\n                    \"stop\": self.get_config_param_value(\"stop_sequences\", is_list=True)[\n                        : self.max_stop_sequences_length\n                    ],\n                    \"frequency_penalty\": self.get_config_param_value(\"frequency_penalty\"),\n                    \"presence_penalty\": self.get_config_param_value(\"presence_penalty\"),\n                })\n            output = AzureChatOpenAI(**base_params)\n        except Exception as e:\n            raise ValueError(f\"Could not connect to AzureOpenAI API: {str(e)}\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"frequency_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"frequency_penalty","display_name":"Frequency Penalty","advanced":true,"dynamic":false,"info":"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":{"text_key":"text","data":{"template":"<file-context>\n{context}\n</file-context>\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nIn addition, take into account the following external context (if any):\n<external-context>\n{external}\n</external-context>\n\nIn addition, take into account the following chat history of the current conversation (if any):\n<chat-history>\n{chat-history}\n</chat-history>\n\nIn addition, it's important to take into account that the user likes to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<liked-answers>\n{liked-answers}\n</liked-answers>\n\nIn addition, it's important to take into account that the user does not like to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<disliked-answers>\n{disliked-answers}\n</disliked-answers>\n\nUser question:\n{user_message}\n\nAI:","variables":{"user_message":{"text_key":"text","data":{"text":"code repo","sender":"User","sender_name":"User","session_id":"61490032-e254-4e3e-a6ae-7dfed508d2de","files":[],"filenames":[],"timestamp":"2025-10-14 09:59:52.632393","flow_id":"a5e564b2-6246-42be-b302-e283295411f0"},"default_value":"","text":"code repo","sender":"User","sender_name":"User","files":[],"session_id":"61490032-e254-4e3e-a6ae-7dfed508d2de","timestamp":"2025-10-14 09:59:52.632393","flow_id":"a5e564b2-6246-42be-b302-e283295411f0","filenames":[]},"context":"","external":"","chat-history":"","liked-answers":"","disliked-answers":"","session_id":"61490032-e254-4e3e-a6ae-7dfed508d2de"},"files":[],"timestamp":"2025-10-14 09:59:52.653306","text":"<file-context>\n\n</file-context>\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nIn addition, take into account the following external context (if any):\n<external-context>\n\n</external-context>\n\nIn addition, take into account the following chat history of the current conversation (if any):\n<chat-history>\n\n</chat-history>\n\nIn addition, it's important to take into account that the user likes to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<liked-answers>\n\n</liked-answers>\n\nIn addition, it's important to take into account that the user does not like to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<disliked-answers>\n\n</disliked-answers>\n\nUser question:\ncode repo\n\nAI:","prompt":{"lc":1,"type":"constructor","id":["langchain","prompts","chat","ChatPromptTemplate"],"kwargs":{"input_variables":[],"messages":[{"content":"<file-context>\n\n</file-context>\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nIn addition, take into account the following external context (if any):\n<external-context>\n\n</external-context>\n\nIn addition, take into account the following chat history of the current conversation (if any):\n<chat-history>\n\n</chat-history>\n\nIn addition, it's important to take into account that the user likes to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<liked-answers>\n\n</liked-answers>\n\nIn addition, it's important to take into account that the user does not like to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<disliked-answers>\n\n</disliked-answers>\n\nUser question:\ncode repo\n\nAI:","additional_kwargs":{},"response_metadata":{},"type":"human","example":false}]},"name":"ChatPromptTemplate"},"messages":[{"content":"<file-context>\n\n</file-context>\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nIn addition, take into account the following external context (if any):\n<external-context>\n\n</external-context>\n\nIn addition, take into account the following chat history of the current conversation (if any):\n<chat-history>\n\n</chat-history>\n\nIn addition, it's important to take into account that the user likes to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<liked-answers>\n\n</liked-answers>\n\nIn addition, it's important to take into account that the user does not like to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<disliked-answers>\n\n</disliked-answers>\n\nUser question:\ncode repo\n\nAI:","additional_kwargs":{},"response_metadata":{},"type":"human","name":null,"id":null,"example":false}],"flow_id":"a5e564b2-6246-42be-b302-e283295411f0"},"default_value":"","text":"<file-context>\n\n</file-context>\nGiven the context above, answer the question as best as possible. For each provided chunk which you used to construct your final answer, include the corresponding provided embedding_id in your answer between html tags like this: <embedding_id>embedding_id</embedding_id>. If the provided chunks are not relevant to the provided question, then don't include, mention, or refer to them in your answer.\n\nIn addition, take into account the following external context (if any):\n<external-context>\n\n</external-context>\n\nIn addition, take into account the following chat history of the current conversation (if any):\n<chat-history>\n\n</chat-history>\n\nIn addition, it's important to take into account that the user likes to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<liked-answers>\n\n</liked-answers>\n\nIn addition, it's important to take into account that the user does not like to see the following answers to their corresponding questions. This information takes precedence over the chat history:\n<disliked-answers>\n\n</disliked-answers>\n\nUser question:\ncode repo\n\nAI:","files":[],"session_id":"","timestamp":"2025-10-14 09:59:52.653306","flow_id":"a5e564b2-6246-42be-b302-e283295411f0","filenames":[]},"name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageInput"},"llm":{"trace_as_metadata":true,"options":["openai-gpt-4o","openai-gpt-4o-mini","openai-gpt-5","openai-o1","openai-o3-mini"],"combobox":false,"required":true,"placeholder":"","show":true,"value":"openai-gpt-4o","name":"llm","display_name":"Deployment Name","advanced":false,"dynamic":false,"info":"","title_case":false,"admin":true,"hidden":false,"default_values":["openai-gpt-35-turbo","openai-gpt-4o","openai-gpt-4o-mini","openai-o1","openai-o3-mini","openai-gpt-5"],"all_possible_options":["openai-gpt-35-turbo","openai-gpt-4o","openai-gpt-4o-mini","openai-o1","openai-o3-mini","openai-gpt-5"],"type":"str","_input_type":"DropdownInput"},"max_tokens":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to use in the generated response. The model truncates the response once the generated text exceeds this value.","title_case":false,"admin":false,"hidden":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"admin":false,"hidden":false,"type":"dict","_input_type":"DictInput"},"not_allowed_topics":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":[],"name":"not_allowed_topics","display_name":"Not Allowed Topics","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Topics in this list are explicitly not allowed. This field takes precedence over the allowed topics.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"presence_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"presence_penalty","display_name":"Presence Penalty","advanced":true,"dynamic":false,"info":"Positive values penalize new tokens based on whether they appear in the text so far, decreasing the model's likelihood to repeat them. Negative values can be used to increase the likelihood of repetition.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"stop_sequences":{"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"stop_sequences","display_name":"Stop Sequences","advanced":true,"dynamic":false,"info":"The model will stop generating further tokens when it encounters any of the stop sequences. The returned text will not contain the stop sequence. The maximum allowed number of stop sequences is 4.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"StrInput"},"store_analytics":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"store_analytics","display_name":"Store Analytics","advanced":true,"dynamic":false,"info":"Whether or not to keep track of LLM analytics.","title_case":false,"admin":false,"hidden":false,"type":"bool","_input_type":"BoolInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"admin":false,"hidden":true,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"temperature","display_name":"Temperature","advanced":true,"dynamic":false,"info":"Controls the randomness of the model output. A higher value means the model will take more risks.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"The model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.","title_case":false,"admin":false,"hidden":false,"type":"float","_input_type":"FloatInput"},"edit":false},"description":"Generate text using Azure OpenAI LLMs.","icon":"Azure","base_classes":["LanguageModel","Message"],"display_name":"Azure OpenAI","documentation":"https://python.langchain.com/docs/integrations/llms/azure_openai","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","allowed_topics","not_allowed_topics","stream","store_analytics","model_kwargs","llm","temperature","max_tokens","top_p","stop_sequences","frequency_penalty","presence_penalty"],"beta":false,"edited":false},"id":"AzureOpenAIModel-5CKr3"},"selected":false,"width":384,"height":480},{"id":"TextOutput-j1brL","type":"genericNode","position":{"x":1593.0816869777425,"y":2141.499892443712},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import os\n\nfrom aisandbox.base.io.text import TextComponent\nfrom aisandbox.graph.vertex.utils import get_provider_model_names\nfrom aisandbox.io import MessageTextInput, Output\nfrom aisandbox.schema.message import Message\n\nfrom sqlmodel import select\nfrom aisandbox.services.deps import session_scope\n\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n        MessageTextInput(\n            name=\"label\",\n            display_name=\"Label\",\n            info=\"Label for reference in the Playground.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        model_id, model_name = None, None\n        # If the text input is provided by an LLM (possibly with a Guardrails component in between), attach its id to the message\n        provider_name, model_name = get_provider_model_names(self.vertex, \"input_value\")\n        if model_name:\n            if not os.getenv(\"EXPORT\", False):\n                from aisandbox.services.database.models.model.utils import get_model_id_by_name\n                model_id = get_model_id_by_name(model_name, provider_name)\n        self.model_id = model_id\n\n        message = Message(text=self.input_value, llm_id=self.model_id)\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"label":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"label","display_name":"Label","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Label for reference in the Playground.","title_case":false,"admin":false,"hidden":false,"type":"str","_input_type":"MessageTextInput"},"edit":false},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","label"],"beta":false,"edited":false},"id":"TextOutput-j1brL"},"selected":false,"width":384,"height":422}],"edges":[{"className":"","data":{"sourceHandle":{"dataType":"ParseData","id":"ParseData-tDuVW","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"context","id":"Prompt-znOhl","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-ParseData-tDuVW{dataType:ParseData,id:ParseData-tDuVW,name:text,output_types:[Message]}-Prompt-znOhl{fieldName:context,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}","selected":false,"source":"ParseData-tDuVW","sourceHandle":"{dataType:ParseData,id:ParseData-tDuVW,name:text,output_types:[Message]}","target":"Prompt-znOhl","targetHandle":"{fieldName:context,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}"},{"className":"","data":{"sourceHandle":{"dataType":"AzureOpenAIEmbeddings","id":"AzureOpenAIEmbeddings-KMgzu","name":"embeddings","output_types":["Embeddings"]},"targetHandle":{"fieldName":"embedding","id":"PGVector-DoOy3","inputTypes":["Embeddings"],"type":"other"}},"id":"reactflow__edge-AzureOpenAIEmbeddings-KMgzu{dataType:AzureOpenAIEmbeddings,id:AzureOpenAIEmbeddings-KMgzu,name:embeddings,output_types:[Embeddings]}-PGVector-DoOy3{fieldName:embedding,id:PGVector-DoOy3,inputTypes:[Embeddings],type:other}","selected":false,"source":"AzureOpenAIEmbeddings-KMgzu","sourceHandle":"{dataType:AzureOpenAIEmbeddings,id:AzureOpenAIEmbeddings-KMgzu,name:embeddings,output_types:[Embeddings]}","target":"PGVector-DoOy3","targetHandle":"{fieldName:embedding,id:PGVector-DoOy3,inputTypes:[Embeddings],type:other}"},{"className":"","data":{"sourceHandle":{"dataType":"AzureOpenAIEmbeddings","id":"AzureOpenAIEmbeddings-91DBt","name":"embeddings","output_types":["Embeddings"]},"targetHandle":{"fieldName":"embedding","id":"PGVector-wLR27","inputTypes":["Embeddings"],"type":"other"}},"id":"reactflow__edge-AzureOpenAIEmbeddings-91DBt{dataType:AzureOpenAIEmbeddings,id:AzureOpenAIEmbeddings-91DBt,name:embeddings,output_types:[Embeddings]}-PGVector-wLR27{fieldName:embedding,id:PGVector-wLR27,inputTypes:[Embeddings],type:other}","selected":false,"source":"AzureOpenAIEmbeddings-91DBt","sourceHandle":"{dataType:AzureOpenAIEmbeddings,id:AzureOpenAIEmbeddings-91DBt,name:embeddings,output_types:[Embeddings]}","target":"PGVector-wLR27","targetHandle":"{fieldName:embedding,id:PGVector-wLR27,inputTypes:[Embeddings],type:other}"},{"className":"","data":{"sourceHandle":{"dataType":"PGVector","id":"PGVector-wLR27","name":"search_results","output_types":["Data"]},"targetHandle":{"fieldName":"data","id":"ParseData-tDuVW","inputTypes":["Data"],"type":"other"}},"id":"reactflow__edge-PGVector-wLR27{dataType:PGVector,id:PGVector-wLR27,name:search_results,output_types:[Data]}-ParseData-tDuVW{fieldName:data,id:ParseData-tDuVW,inputTypes:[Data],type:other}","selected":false,"source":"PGVector-wLR27","sourceHandle":"{dataType:PGVector,id:PGVector-wLR27,name:search_results,output_types:[Data]}","target":"ParseData-tDuVW","targetHandle":"{fieldName:data,id:ParseData-tDuVW,inputTypes:[Data],type:other}"},{"className":"","data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-znOhl","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"AzureOpenAIModel-HAO12","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-Prompt-znOhl{dataType:Prompt,id:Prompt-znOhl,name:prompt,output_types:[Message]}-AzureOpenAIModel-HAO12{fieldName:input_value,id:AzureOpenAIModel-HAO12,inputTypes:[Message],type:str}","selected":false,"source":"Prompt-znOhl","sourceHandle":"{dataType:Prompt,id:Prompt-znOhl,name:prompt,output_types:[Message]}","target":"AzureOpenAIModel-HAO12","targetHandle":"{fieldName:input_value,id:AzureOpenAIModel-HAO12,inputTypes:[Message],type:str}"},{"className":"","data":{"sourceHandle":{"dataType":"AzureOpenAIModel","id":"AzureOpenAIModel-HAO12","name":"text_output","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"ChatOutput-0CMXD","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-AzureOpenAIModel-HAO12{dataType:AzureOpenAIModel,id:AzureOpenAIModel-HAO12,name:text_output,output_types:[Message]}-ChatOutput-0CMXD{fieldName:input_value,id:ChatOutput-0CMXD,inputTypes:[Message],type:str}","selected":false,"source":"AzureOpenAIModel-HAO12","sourceHandle":"{dataType:AzureOpenAIModel,id:AzureOpenAIModel-HAO12,name:text_output,output_types:[Message]}","target":"ChatOutput-0CMXD","targetHandle":"{fieldName:input_value,id:ChatOutput-0CMXD,inputTypes:[Message],type:str}"},{"data":{"sourceHandle":{"dataType":"S3File","id":"S3File-67R8g","name":"data","output_types":["Data"]},"targetHandle":{"fieldName":"data_input","id":"RecursiveCharacterTextSplitter-TybcH","inputTypes":["Document","Data"],"type":"other"}},"id":"reactflow__edge-S3File-67R8g{dataType:S3File,id:S3File-67R8g,name:data,output_types:[Data]}-RecursiveCharacterTextSplitter-TybcH{fieldName:data_input,id:RecursiveCharacterTextSplitter-TybcH,inputTypes:[Document,Data],type:other}","source":"S3File-67R8g","sourceHandle":"{dataType:S3File,id:S3File-67R8g,name:data,output_types:[Data]}","target":"RecursiveCharacterTextSplitter-TybcH","targetHandle":"{fieldName:data_input,id:RecursiveCharacterTextSplitter-TybcH,inputTypes:[Document,Data],type:other}","className":""},{"data":{"sourceHandle":{"dataType":"RecursiveCharacterTextSplitter","id":"RecursiveCharacterTextSplitter-TybcH","name":"data","output_types":["Data"]},"targetHandle":{"fieldName":"ingest_data","id":"PGVector-DoOy3","inputTypes":["Data"],"type":"other"}},"id":"reactflow__edge-RecursiveCharacterTextSplitter-TybcH{dataType:RecursiveCharacterTextSplitter,id:RecursiveCharacterTextSplitter-TybcH,name:data,output_types:[Data]}-PGVector-DoOy3{fieldName:ingest_data,id:PGVector-DoOy3,inputTypes:[Data],type:other}","source":"RecursiveCharacterTextSplitter-TybcH","sourceHandle":"{dataType:RecursiveCharacterTextSplitter,id:RecursiveCharacterTextSplitter-TybcH,name:data,output_types:[Data]}","target":"PGVector-DoOy3","targetHandle":"{fieldName:ingest_data,id:PGVector-DoOy3,inputTypes:[Data],type:other}","className":""},{"source":"Memory-EElXY","sourceHandle":"{dataType:Memory,id:Memory-EElXY,name:messages_text,output_types:[Message]}","target":"Prompt-znOhl","targetHandle":"{fieldName:Memory,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"Memory","id":"Prompt-znOhl","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Memory","id":"Memory-EElXY","name":"messages_text","output_types":["Message"]}},"id":"reactflow__edge-Memory-EElXY{dataType:Memory,id:Memory-EElXY,name:messages_text,output_types:[Message]}-Prompt-znOhl{fieldName:Memory,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}","selected":false,"className":""},{"source":"ChatInput-yKBP3","sourceHandle":"{dataType:ChatInput,id:ChatInput-yKBP3,name:message,output_types:[Message]}","target":"Guardrails-OBiSc","targetHandle":"{fieldName:text,id:Guardrails-OBiSc,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"text","id":"Guardrails-OBiSc","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-yKBP3","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-yKBP3{dataType:ChatInput,id:ChatInput-yKBP3,name:message,output_types:[Message]}-Guardrails-OBiSc{fieldName:text,id:Guardrails-OBiSc,inputTypes:[Message],type:str}","selected":false,"className":""},{"source":"Guardrails-OBiSc","sourceHandle":"{dataType:Guardrails,id:Guardrails-OBiSc,name:true_result,output_types:[Message]}","target":"Prompt-znOhl","targetHandle":"{fieldName:question,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-znOhl","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Guardrails","id":"Guardrails-OBiSc","name":"true_result","output_types":["Message"]}},"id":"reactflow__edge-Guardrails-OBiSc{dataType:Guardrails,id:Guardrails-OBiSc,name:true_result,output_types:[Message]}-Prompt-znOhl{fieldName:question,id:Prompt-znOhl,inputTypes:[Message,Text],type:str}","className":""},{"source":"Guardrails-OBiSc","sourceHandle":"{dataType:Guardrails,id:Guardrails-OBiSc,name:false_result,output_types:[Message]}","target":"ChatOutput-2jCcY","targetHandle":"{fieldName:input_value,id:ChatOutput-2jCcY,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-2jCcY","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Guardrails","id":"Guardrails-OBiSc","name":"false_result","output_types":["Message"]}},"id":"reactflow__edge-Guardrails-OBiSc{dataType:Guardrails,id:Guardrails-OBiSc,name:false_result,output_types:[Message]}-ChatOutput-2jCcY{fieldName:input_value,id:ChatOutput-2jCcY,inputTypes:[Message],type:str}","className":""},{"source":"AzureOpenAIModel-5CKr3","sourceHandle":"{dataType:AzureOpenAIModel,id:AzureOpenAIModel-5CKr3,name:model_output,output_types:[LanguageModel]}","target":"Guardrails-OBiSc","targetHandle":"{fieldName:llm,id:Guardrails-OBiSc,inputTypes:[LanguageModel],type:other}","data":{"targetHandle":{"fieldName":"llm","id":"Guardrails-OBiSc","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"AzureOpenAIModel","id":"AzureOpenAIModel-5CKr3","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-AzureOpenAIModel-5CKr3{dataType:AzureOpenAIModel,id:AzureOpenAIModel-5CKr3,name:model_output,output_types:[LanguageModel]}-Guardrails-OBiSc{fieldName:llm,id:Guardrails-OBiSc,inputTypes:[LanguageModel],type:other}","className":""},{"source":"Guardrails-OBiSc","sourceHandle":"{dataType:Guardrails,id:Guardrails-OBiSc,name:true_result,output_types:[Message]}","target":"PGVector-wLR27","targetHandle":"{fieldName:search_query,id:PGVector-wLR27,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"search_query","id":"PGVector-wLR27","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Guardrails","id":"Guardrails-OBiSc","name":"true_result","output_types":["Message"]}},"id":"reactflow__edge-Guardrails-OBiSc{dataType:Guardrails,id:Guardrails-OBiSc,name:true_result,output_types:[Message]}-PGVector-wLR27{fieldName:search_query,id:PGVector-wLR27,inputTypes:[Message],type:str}","className":""}],"viewport":{"x":687.8378322968105,"y":-329.52429825671265,"zoom":0.5239276018747605}},"description":"Credit Risk Report Generation by analyst","name":"Credit Risk Report (1)","last_tested_version":"0.0.92","endpoint_name":null,"is_component":false}